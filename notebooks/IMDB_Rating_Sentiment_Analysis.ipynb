{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e51efef",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8008a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import emoji\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import spmatrix\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Ignore all runtime warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7a7b6",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908d8ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_ratings_df = pd.read_csv(filepath_or_buffer = '../data/IMDB_Dataset.csv',\n",
    "                              index_col          = None)\n",
    "\n",
    "imdb_ratings_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a1aa6",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf03acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing text data through cleaning, tokenization, and normalization\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        lemmatizer : WordNetLemmatizer instance for word lemmatization\n",
    "        \n",
    "        stop_words : Set of stopwords to be removed from text\n",
    "    \"\"\" \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the TextPreprocessor with required NLTK resources\n",
    "        \n",
    "        Raises:\n",
    "        -------\n",
    "            LookupError : If required NLTK resources cannot be downloaded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Download required NLTK data\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)\n",
    "            \n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "        except LookupError as e:\n",
    "            raise\n",
    "    \n",
    "    def clean_text(self, text:str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize input text by removing HTML tags, special characters,\n",
    "        and applying text normalization techniques\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            text { str }      : Input text to be cleaned\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError        : If input text is None or empty\n",
    "            \n",
    "            TextCleaningError : If any error occurs at any step of text cleaning process\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "                { str }       : Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        if ((not text) or (not isinstance(text, str))):\n",
    "            raise ValueError(\"Input text must be a non-empty string\")\n",
    "            \n",
    "        try:\n",
    "            # Remove HTML tags\n",
    "            text   = re.sub('<[^>]*>', '', text)\n",
    "            \n",
    "            # Remove special characters and digits\n",
    "            text   = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            text   = text.lower()\n",
    "            \n",
    "            # Tokenization\n",
    "            tokens = word_tokenize(text)\n",
    "            \n",
    "            # Remove stopwords and lemmatize\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "            \n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        except Exception as TextCleaningError:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a0ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor\n",
    "preprocessor                  = TextPreprocessor()\n",
    "\n",
    "# Add a new column to the original DataFrame to store the cleaned texts\n",
    "imdb_ratings_df['clean_text'] = imdb_ratings_df['review'].apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d75ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>probably alltime favorite movie story selfless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sure would like see resurrection dated seahunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>show amazing fresh innovative idea first aired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>encouraged positive comment film looking forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>like original gut wrenching laughter like movi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "5  Probably my all-time favorite movie, a story o...  positive   \n",
       "6  I sure would like to see a resurrection of a u...  positive   \n",
       "7  This show was an amazing, fresh & innovative i...  negative   \n",
       "8  Encouraged by the positive comments about this...  negative   \n",
       "9  If you like original gut wrenching laughter yo...  positive   \n",
       "\n",
       "                                          clean_text  \n",
       "0  one reviewer mentioned watching oz episode you...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically there family little boy jake think t...  \n",
       "4  petter matteis love time money visually stunni...  \n",
       "5  probably alltime favorite movie story selfless...  \n",
       "6  sure would like see resurrection dated seahunt...  \n",
       "7  show amazing fresh innovative idea first aired...  \n",
       "8  encouraged positive comment film looking forwa...  \n",
       "9  like original gut wrenching laughter like movi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings After Cleaning\n",
    "imdb_ratings_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c2806",
   "metadata": {},
   "source": [
    "## Exploratyory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12c0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentEDA:\n",
    "    \"\"\"\n",
    "    A class for comprehensive Exploratory Data Analysis of sentiment-based text data\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        df            { DataFrame } : DataFrame containing text and sentiment data\n",
    "        \n",
    "        text_column      { str }    : Name of column containing cleaned text\n",
    "        \n",
    "        sentiment_column { str }    : Name of column containing sentiment labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, text_column:str = 'clean_text', sentiment_column:str = 'sentiment',\n",
    "                 output_dir:str = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SentimentEDA class\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            df           { DataFrame } : Input DataFrame\n",
    "            \n",
    "            text_column      { str }   : Name of text column\n",
    "            \n",
    "            sentiment_column { str }   : Name of sentiment column\n",
    "            \n",
    "            output_dir       { str }   : Directory to save plots\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError                 : If required columns are not in DataFrame\n",
    "        \"\"\"\n",
    "        if ((text_column not in df.columns) or (sentiment_column not in df.columns)):\n",
    "            raise ValueError(f\"DataFrame must contain columns: {text_column} and {sentiment_column}\")\n",
    "        \n",
    "        # Initialize the Attributes\n",
    "        self.df               = df\n",
    "        self.text_column      = text_column\n",
    "        self.sentiment_column = sentiment_column\n",
    "        self.output_dir       = output_dir\n",
    "        \n",
    "        if output_dir:\n",
    "            Path(output_dir).mkdir(parents  = True, \n",
    "                                   exist_ok = True)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def save_plot(self, plt: plt, plot_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Helper method to save plots if output directory is specified\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.output_dir:\n",
    "            plt.savefig(fname       = f\"{self.output_dir}/{plot_name}.png\", \n",
    "                        bbox_inches = 'tight', \n",
    "                        dpi         = 300)\n",
    "            \n",
    "            # Print the success statement on screen\n",
    "            print(f\"Saved plot: {plot_name}\")\n",
    "    \n",
    "    \n",
    "    def text_length_analysis(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze and visualize text length distributions\n",
    "        \n",
    "        - Character length distribution\n",
    "        \n",
    "        - Word length distribution\n",
    "        \n",
    "        - Sentence length distribution\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate lengths\n",
    "            self.df['char_length']     = self.df[self.text_column].str.len()\n",
    "            self.df['word_length']     = self.df[self.text_column].str.split().str.len()\n",
    "            self.df['sentence_length'] = self.df[self.text_column].str.split('.').str.len()\n",
    "            \n",
    "            # Create subplots\n",
    "            fig, axes                  = plt.subplots(nrows   = 3, \n",
    "                                                      ncols   = 1, \n",
    "                                                      figsize = (12, 15))\n",
    "            \n",
    "            # Character length\n",
    "            sns.boxplot(x    = self.sentiment_column, \n",
    "                        y    = 'char_length', \n",
    "                        data = self.df, \n",
    "                        ax   = axes[0])\n",
    "            \n",
    "            axes[0].set_title('Character Length Distribution by Sentiment')\n",
    "            axes[0].set_ylabel('Number of Characters')\n",
    "            \n",
    "            # Word length\n",
    "            sns.boxplot(x    = self.sentiment_column, \n",
    "                        y    = 'word_length', \n",
    "                        data = self.df, \n",
    "                        ax   = axes[1])\n",
    "            \n",
    "            axes[1].set_title('Word Length Distribution by Sentiment')\n",
    "            axes[1].set_ylabel('Number of Words')\n",
    "            \n",
    "            # Sentence length\n",
    "            sns.boxplot(x    = self.sentiment_column, \n",
    "                        y    = 'sentence_length', \n",
    "                        data = self.df, \n",
    "                        ax   = axes[2])\n",
    "            \n",
    "            axes[2].set_title('Sentence Length Distribution by Sentiment')\n",
    "            axes[2].set_ylabel('Number of Sentences')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.save_plot(plt, 'length_distributions')\n",
    "            plt.close()\n",
    "            \n",
    "            # Print summary statistics\n",
    "            stats = self.df.groupby(self.sentiment_column)[['char_length', \n",
    "                                                            'word_length', \n",
    "                                                            'sentence_length']].describe()\n",
    "            \n",
    "            print(\"\\nLength Statistics by Sentiment:\")\n",
    "            print(f\"\\n{stats}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def word_frequency_analysis(self, top_n: int = 20) -> None:\n",
    "        \"\"\"\n",
    "        Analyze and visualize word frequencies by sentiment\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            top_n { int } : Number of top words to display\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Split by sentiment\n",
    "            sentiment_texts  = {sentiment: ' '.join(group[self.text_column]) for sentiment, group \\\n",
    "                                in self.df.groupby(self.sentiment_column)}\n",
    "            \n",
    "            # Create word frequency plots for each sentiment\n",
    "            fig, axes        = plt.subplots(nrows   = len(sentiment_texts), \n",
    "                                            ncols   = 1, \n",
    "                                            figsize = (12, 5*len(sentiment_texts)))\n",
    "            \n",
    "            for idx, (sentiment, text) in enumerate(sentiment_texts.items()):\n",
    "                words        = text.split()\n",
    "                word_freq    = Counter(words).most_common(top_n)\n",
    "                words, freqs = zip(*word_freq)\n",
    "                \n",
    "                ax           = axes[idx] if len(sentiment_texts) > 1 else axes\n",
    "                \n",
    "                sns.barplot(x  = list(freqs), \n",
    "                            y  = list(words), \n",
    "                            ax = ax)\n",
    "                \n",
    "                ax.set_title(f'Top {top_n} Words for {sentiment} Sentiment')\n",
    "                ax.set_xlabel('Frequency')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            self.save_plot(plt, 'word_frequencies')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def generate_wordclouds(self) -> None:\n",
    "        \"\"\"\n",
    "        Generate and display wordclouds for each sentiment category\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create wordcloud for each sentiment\n",
    "            sentiment_texts = {sentiment: ' '.join(group[self.text_column]) for sentiment, group in self.df.groupby(self.sentiment_column)}\n",
    "            \n",
    "            fig, axes       = plt.subplots(1, len(sentiment_texts), figsize=(15, 8))\n",
    "            \n",
    "            for idx, (sentiment, text) in enumerate(sentiment_texts.items()):\n",
    "                wordcloud = WordCloud(width            = 800, \n",
    "                                      height           = 400,\n",
    "                                      background_color = 'white',\n",
    "                                      max_words        = 150).generate(text)\n",
    "                \n",
    "                ax        = axes[idx] if len(sentiment_texts) > 1 else axes\n",
    "                \n",
    "                ax.imshow(wordcloud, interpolation='bilinear')\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'{sentiment} Sentiment WordCloud')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            self.save_plot(plt, 'wordclouds')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def sentiment_intensity_analysis(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze sentiment intensity using TextBlob's polarity scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate polarity scores\n",
    "            self.df['polarity'] = self.df[self.text_column].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "            \n",
    "            # Create distribution plot\n",
    "            plt.figure(figsize = (10, 6))\n",
    "            \n",
    "            sns.kdeplot(data = self.df, \n",
    "                        x    = 'polarity', \n",
    "                        hue  = self.sentiment_column)\n",
    "            \n",
    "            plt.title('Sentiment Polarity Distribution')\n",
    "            plt.xlabel('Polarity Score')\n",
    "            plt.ylabel('Density')\n",
    "\n",
    "            self.save_plot(plt, 'sentiment_intensity')\n",
    "            plt.close()\n",
    "            \n",
    "            # Print summary statistics\n",
    "            stats = self.df.groupby(self.sentiment_column)['polarity'].describe()\n",
    "            print(\"\\nPolarity Statistics by Sentiment:\")\n",
    "            print(f\"\\n{stats}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "        \n",
    "        \n",
    "    def pos_distribution_analysis(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze distribution of Parts of Speech across sentiments\n",
    "        \"\"\"\n",
    "        try:\n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "            \n",
    "            def get_pos_counts(text: str):\n",
    "                doc = nlp(text)\n",
    "                return Counter([token.pos_ for token in doc])\n",
    "            \n",
    "            # Get POS counts for sample of texts (for efficiency)\n",
    "            sample_size                   = min(1000, len(self.df))\n",
    "            sample_df                     = self.df.sample(sample_size, random_state=42)\n",
    "            \n",
    "            pos_counts                    = sample_df[self.text_column].apply(get_pos_counts)\n",
    "            pos_df                        = pd.DataFrame(pos_counts.tolist())\n",
    "            pos_df[self.sentiment_column] = sample_df[self.sentiment_column]\n",
    "            \n",
    "            # Create visualization\n",
    "            pos_melted                    = pos_df.melt(id_vars    = [self.sentiment_column],\n",
    "                                                        var_name   = 'POS',\n",
    "                                                        value_name = 'Count')\n",
    "            \n",
    "            plt.figure(figsize = (12, 6))\n",
    "            sns.boxplot(x    = 'POS', \n",
    "                        y    = 'Count', \n",
    "                        hue  = self.sentiment_column, \n",
    "                        data = pos_melted)\n",
    "            \n",
    "            plt.xticks(rotation = 45)\n",
    "            plt.title('Distribution of Parts of Speech by Sentiment')\n",
    "    \n",
    "            self.save_plot(plt, 'pos_distribution')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def analyze_readability(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze text readability metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            def calculate_readability(text: str):\n",
    "                words               = text.split()\n",
    "                sentences           = text.split('.')\n",
    "                \n",
    "                # Rough approximation\n",
    "                syllables           = sum([len(word)/3 for word in words])  \n",
    "                \n",
    "                # Calculate metrics\n",
    "                avg_word_length     = np.mean([len(word) for word in words])\n",
    "                avg_sentence_length = len(words) / len(sentences)\n",
    "                flesch_reading_ease = 206.835 - 1.015 * avg_sentence_length - 84.6 * (syllables / len(words))\n",
    "                \n",
    "                return {'avg_word_length': avg_word_length,\n",
    "                        'avg_sentence_length': avg_sentence_length,\n",
    "                        'flesch_reading_ease': flesch_reading_ease\n",
    "                       }\n",
    "            \n",
    "            # Calculate readability metrics\n",
    "            readability_scores                    = self.df[self.text_column].apply(calculate_readability)\n",
    "            readability_df                        = pd.DataFrame(readability_scores.tolist())\n",
    "            readability_df[self.sentiment_column] = self.df[self.sentiment_column]\n",
    "            \n",
    "            # Create visualizations\n",
    "            fig, axes                             = plt.subplots(nrows   = 3, \n",
    "                                                                 ncols   = 1, \n",
    "                                                                 figsize = (10, 15))\n",
    "            \n",
    "            for idx, metric in enumerate(readability_df.columns[:-1]):\n",
    "                sns.boxplot(x    = self.sentiment_column,\n",
    "                            y    = metric,\n",
    "                            data = readability_df,\n",
    "                            ax   = axes[idx])\n",
    "                \n",
    "                axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()} by Sentiment')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            self.save_plot(plt, 'readability_metrics')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "         \n",
    "        \n",
    "    def run_full_eda(self) -> None:\n",
    "        \"\"\"\n",
    "        Run all EDA analyses\n",
    "        \n",
    "        \"\"\"\n",
    "        analyses = [self.text_length_analysis,\n",
    "                    self.word_frequency_analysis,\n",
    "                    self.generate_wordclouds,\n",
    "                    self.sentiment_intensity_analysis,\n",
    "                    self.pos_distribution_analysis,\n",
    "                    self.analyze_readability\n",
    "                   ]\n",
    "        \n",
    "        for analysis in analyses:\n",
    "            print(f\"Running {analysis.__name__}...\")\n",
    "            analysis()\n",
    "            \n",
    "        print(\"EDA pipeline completed successfully\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33befd26",
   "metadata": {},
   "source": [
    "### Initialize the EDA class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a381e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = SentimentEDA(df               = imdb_ratings_df,\n",
    "                   text_column      = 'clean_text',\n",
    "                   sentiment_column = 'sentiment',\n",
    "                   output_dir       = '../results/EDA_Results/')\n",
    "\n",
    "# Or run all analyses at once\n",
    "#eda.run_full_eda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e27296",
   "metadata": {},
   "source": [
    "### Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79230558",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.text_length_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5845ae5",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29918509",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.word_frequency_analysis(top_n = 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf38f6",
   "metadata": {},
   "source": [
    "### WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.generate_wordclouds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572832c",
   "metadata": {},
   "source": [
    "### Sentiment Intensity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64442cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.sentiment_intensity_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a0a5e",
   "metadata": {},
   "source": [
    "### POS Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.pos_distribution_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3d05b",
   "metadata": {},
   "source": [
    "### Readability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.analyze_readability()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c377b704",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f333ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureEngineering:\n",
    "    \"\"\"\n",
    "    A class for implementing various text feature engineering techniques\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        texts        { list }  : List of preprocessed text documents\n",
    "        \n",
    "        max_features  { int }  : Maximum number of features to create\n",
    "        \n",
    "        ngram_range  { tuple } : Range of n-grams to consider\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, max_features: int = None, ngram_range: tuple = (1, 3)) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureEngineering with texts and parameters\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            texts        : List of preprocessed text documents\n",
    "            \n",
    "            max_features : Maximum number of features (None for no limit)\n",
    "            \n",
    "            ngram_range  : Range of n-grams to consider (min_n, max_n)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError   : If texts is empty or parameters are invalid\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            raise ValueError(\"Input texts cannot be empty\")\n",
    "            \n",
    "        self.texts        = texts\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range  = ngram_range\n",
    "        \n",
    "        \n",
    "    def create_binary_bow(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create binary bag-of-words features (presence/absence)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer\n",
    "                                          - Binary document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating binary bag-of-words features...\")\n",
    "            vectorizer = CountVectorizer(binary       = True,\n",
    "                                         max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} binary features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_count_bow(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create count-based bag-of-words features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer\n",
    "                                          - Count document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating count-based bag-of-words features...\")\n",
    "            vectorizer = CountVectorizer(max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} count-based features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_frequency_bow(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create frequency-based bag-of-words features (term frequency)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted TfidfVectorizer\n",
    "                                          - Term frequency document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating frequency-based bag-of-words features...\")\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(use_idf      = False,\n",
    "                                         max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} frequency-based features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_tfidf(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create TF-IDF features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted TfidfVectorizer\n",
    "                                          - TF-IDF document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating TF-IDF features...\")\n",
    "            vectorizer = TfidfVectorizer(max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} TF-IDF features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_standardized_tfidf(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create Standardized TF-IDF features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted TfidfVectorizer\n",
    "                                          - Standardized TF-IDF document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Standardized TF-IDF features...\")\n",
    "            vectorizer          = TfidfVectorizer(max_features = self.max_features, \n",
    "                                                  ngram_range  = self.ngram_range)\n",
    "            \n",
    "            tfidf_matrix        = vectorizer.fit_transform(self.texts)\n",
    "            \n",
    "            scaler              = StandardScaler(with_mean = False)\n",
    "            \n",
    "            standardized_matrix = scaler.fit_transform(tfidf_matrix)\n",
    "            \n",
    "            print(f\"Created {standardized_matrix.shape[1]} standardized TF-IDF features\")\n",
    "            return vectorizer, standardized_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def _create_bm25_variant(self, variant: str, k1: float = 1.5, b: float = 0.75, delta: float = 1.0) -> tuple:\n",
    "        \"\"\"\n",
    "        Unified private method to create BM25 variant features.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            variant : Specify the BM25 variant (\"BM25\", \"BM25F\", \"BM25L\", \"BM25+\", \"BM25T\")\n",
    "            k1      : Term frequency saturation parameter (default: 1.5)\n",
    "            b       : Length normalization parameter (default: 0.75)\n",
    "            delta   : Free parameter for certain variants (default: 1.0)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing:\n",
    "                        - Custom transformer for the specified BM25 variant\n",
    "                        - BM25 variant document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Creating {variant} features...\")\n",
    "\n",
    "            class BM25VariantTransformer(BaseEstimator, TransformerMixin):\n",
    "                def __init__(self, k1=1.5, b=0.75, delta=1.0, variant=\"BM25\"):\n",
    "                    self.k1               = k1\n",
    "                    self.b                = b\n",
    "                    self.delta            = delta\n",
    "                    self.variant          = variant\n",
    "                    self.count_vectorizer = CountVectorizer()\n",
    "\n",
    "                def fit(self, texts):\n",
    "                    # Calculate IDF and average document length\n",
    "                    X                   = self.count_vectorizer.fit_transform(texts)\n",
    "                    self.avg_doc_length = X.sum(axis=1).mean()\n",
    "                    n_docs              = len(texts)\n",
    "                    df                  = np.bincount(X.indices, minlength=X.shape[1])\n",
    "                    self.idf            = np.log((n_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "                    return self\n",
    "\n",
    "                def transform(self, texts):\n",
    "                    # Calculate BM25 variant scores\n",
    "                    X           = self.count_vectorizer.transform(texts)\n",
    "                    doc_lengths = X.sum(axis=1).A1\n",
    "                    rows, cols  = X.nonzero()\n",
    "                    data        = list()\n",
    "\n",
    "                    for i, j in zip(rows, cols):\n",
    "                        tf = X[i, j]\n",
    "                        if (self.variant == \"BM25\"):\n",
    "                            numerator   = tf * (self.k1 + 1)\n",
    "                            denominator = tf + self.k1 * (1 - self.b + self.b * doc_lengths[i] / self.avg_doc_length)\n",
    "                            score       = self.idf[j] * numerator / denominator\n",
    "                        \n",
    "                        elif self.variant == \"BM25F\":\n",
    "                            score = self.idf[j] * (tf / (self.k1 + tf))\n",
    "                            \n",
    "                        elif (self.variant == \"BM25L\"):\n",
    "                            numerator   = tf + self.delta\n",
    "                            denominator = tf + self.delta + self.k1 * (1 - self.b + self.b * doc_lengths[i] / self.avg_doc_length)\n",
    "                            score       = self.idf[j] * numerator / denominator\n",
    "                        \n",
    "                        elif (self.variant == \"BM25+\"):\n",
    "                            numerator   = tf + self.delta\n",
    "                            denominator = tf + self.k1\n",
    "                            score       = self.idf[j] * numerator / denominator\n",
    "                        \n",
    "                        elif (self.variant == \"BM25T\"):\n",
    "                            score = self.idf[j] * (tf * np.log(1 + tf))\n",
    "                        \n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown variant: {self.variant}\")\n",
    "                        \n",
    "                        data.append(score)\n",
    "\n",
    "                    return csr_matrix((data, (rows, cols)), shape=X.shape)\n",
    "\n",
    "            transformer = BM25VariantTransformer(k1      = k1, \n",
    "                                                 b       = b, \n",
    "                                                 delta   = delta, \n",
    "                                                 variant = variant)\n",
    "            features    = transformer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} {variant} features\")\n",
    "            return transformer, features\n",
    "\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    def create_bm25(self, k1: float = 1.5, b: float = 0.75) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25 features.\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant=\"BM25\", k1=k1, b=b)\n",
    "\n",
    "\n",
    "    def create_bm25f(self, k1: float = 1.5) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25F features.\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant=\"BM25F\", k1=k1)\n",
    "\n",
    "\n",
    "    def create_bm25l(self, k1: float = 1.5, b: float = 0.75, delta: float = 1.0) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25L features.\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant=\"BM25L\", k1=k1, b=b, delta=delta)\n",
    "\n",
    "\n",
    "    def create_bm25_plus(self, k1: float = 1.5, delta: float = 1.0) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25+ features.\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant=\"BM25+\", k1=k1, delta=delta)\n",
    "\n",
    "\n",
    "    def create_bm25t(self, k1: float = 1.5) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25T features.\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant=\"BM25T\", k1=k1)\n",
    "\n",
    "\n",
    "    def create_skipgrams(self, k: int = 2) -> tuple:\n",
    "        \"\"\"\n",
    "        Create skipgram features\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            k { int } : Skip distance\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer for skipgrams\n",
    "                                          - Skipgram document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating skipgram features...\")\n",
    "            \n",
    "            def generate_skipgrams(text: str) -> str:\n",
    "                words     = text.split()\n",
    "                skipgrams = list()\n",
    "                \n",
    "                for i in range(len(words) - k - 1):\n",
    "                    skipgram = f\"{words[i]}_{words[i + k + 1]}\"\n",
    "                    skipgrams.append(skipgram)\n",
    "                    \n",
    "                return ' '.join(skipgrams)\n",
    "            \n",
    "            processed_texts = [generate_skipgrams(text) for text in self.texts]\n",
    "            \n",
    "            vectorizer      = CountVectorizer(max_features=self.max_features)\n",
    "            features        = vectorizer.fit_transform(processed_texts)\n",
    "            \n",
    "            print(f\"Created {features.shape[1]} skipgram features\")\n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_positional_ngrams(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create positional n-gram features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer for positional n-grams\n",
    "                                          - Positional n-gram document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating positional n-gram features...\")\n",
    "            \n",
    "            def generate_positional_ngrams(text: str) -> str:\n",
    "                words      = text.split()\n",
    "                pos_ngrams = list()\n",
    "                \n",
    "                for i in range(len(words)):\n",
    "                    for n in range(self.ngram_range[0], min(self.ngram_range[1] + 1, len(words) - i + 1)):\n",
    "                        ngram     = '_'.join(words[i:i+n])\n",
    "                        pos_ngram = f\"pos{i}_{ngram}\"\n",
    "                        pos_ngrams.append(pos_ngram)\n",
    "                        \n",
    "                return ' '.join(pos_ngrams)\n",
    "            \n",
    "            processed_texts = [generate_positional_ngrams(text) for text in self.texts]\n",
    "            \n",
    "            vectorizer      = CountVectorizer(max_features = self.max_features)\n",
    "            \n",
    "            features        = vectorizer.fit_transform(processed_texts)\n",
    "            \n",
    "            print(f\"Created {features.shape[1]} positional n-gram features\")\n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_all_features(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create all available feature types\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { dict } : Dictionary mapping feature names to their vectorizer and feature matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating all feature types...\")\n",
    "            features                      = dict()\n",
    "            \n",
    "            # Create all feature types\n",
    "            features['binary_bow']        = self.create_binary_bow()\n",
    "            features['count_bow']         = self.create_count_bow()\n",
    "            features['frequency_bow']     = self.create_frequency_bow()\n",
    "            features['tfidf']             = self.create_tfidf()\n",
    "            features['bm25']              = self.create_bm25()\n",
    "            features['skipgrams']         = self.create_skipgrams()\n",
    "            features['positional_ngrams'] = self.create_positional_ngrams()\n",
    "            \n",
    "            print(\"Created all feature types successfully\")\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fa580",
   "metadata": {},
   "source": [
    "### Initialize the feature engineering class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_eng = TextFeatureEngineering(texts        = imdb_ratings_df['clean_text'].tolist(),\n",
    "                                     max_features = None,\n",
    "                                     ngram_range  = (1, 3)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1411b6",
   "metadata": {},
   "source": [
    "### Create specific feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature matrices\n",
    "#count_vectorizer, count_features          = feature_eng.create_count_bow()\n",
    "#freq_vectorizer, freq_features            = feature_eng.create_frequency_bow()\n",
    "#binary_vectorizer, binary_features        = feature_eng.create_binary_bow()\n",
    "#tfidf_vectorizer, tfidf_features          = feature_eng.create_tfidf()\n",
    "std_tfidf_vectorizer, std_tfidf_features  = feature_eng.create_standardized_tfidf()\n",
    "#bm25_transformer, bm25_features           = feature_eng.create_bm25()\n",
    "bm25f_transformer, bm25f_features         = feature_eng.create_bm25f()\n",
    "#bm25l_transformer, bm25l_features         = feature_eng.create_bm25l()\n",
    "#bm25t_transformer, bm25t_features         = feature_eng.create_bm25t()\n",
    "bm25_plus_transformer, bm25_plus_features = feature_eng.create_bm25_plus()\n",
    "skipgrams_vectorizer, skipgram_features   = feature_eng.create_skipgrams()\n",
    "pos_ngram_vectorizer, pos_ngram_features  = feature_eng.create_positional_ngrams()\n",
    "\n",
    "# Combine feature matrices\n",
    "combined_features                         = hstack([#count_features, \n",
    "                                                    #freq_features, \n",
    "                                                    #binary_features, \n",
    "                                                    #tfidf_features, \n",
    "                                                    std_tfidf_features,\n",
    "                                                    #bm25_features,\n",
    "                                                    bm25f_features,\n",
    "                                                    #bm25l_features,\n",
    "                                                    #bm25t_features,\n",
    "                                                    bm25_plus_features,\n",
    "                                                    skipgram_features,\n",
    "                                                    pos_ngram_features])\n",
    "\n",
    "# Combine feature names\n",
    "feature_names                             = (#list(count_vectorizer.get_feature_names_out()) +\n",
    "                                             #list(freq_vectorizer.get_feature_names_out()) +\n",
    "                                             #list(binary_vectorizer.get_feature_names_out()) +\n",
    "                                             #list(tfidf_vectorizer.get_feature_names_out()) +\n",
    "                                             list(std_tfidf_vectorizer.get_feature_names_out()) +\n",
    "                                             #list(bm25_transformer.count_vectorizer.get_feature_names_out()) +\n",
    "                                             list(bm25f_transformer.count_vectorizer.get_feature_names_out()) +\n",
    "                                             #list(bm25l_transformer.count_vectorizer.get_feature_names_out()) +\n",
    "                                             #list(bm25t_transformer.count_vectorizer.get_feature_names_out()) +\n",
    "                                             list(bm25_plus_transformer.count_vectorizer.get_feature_names_out()) +\n",
    "                                             list(skipgrams_vectorizer.get_feature_names_out()) +\n",
    "                                             list(pos_ngram_vectorizer.get_feature_names_out())\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or create all feature types at once\n",
    "# all_features = feature_eng.create_all_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045d1fd",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b389abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureSelector:\n",
    "    \"\"\"\n",
    "    A class for implementing various feature selection techniques for text data\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        X           { spmatrix } : Feature matrix\n",
    "        \n",
    "        y           { ndarray }  : Target labels\n",
    "\n",
    "        feature_names { list }   : Names of features\n",
    "        \n",
    "        n_features    { int }    : Number of features to select\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X: spmatrix, y: np.ndarray, feature_names: list, n_features: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureSelector with feature matrix and labels\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            X             : Sparse feature matrix\n",
    "            \n",
    "            y             : Target labels\n",
    "            \n",
    "            feature_names : List of feature names\n",
    "            \n",
    "            n_features    : Number of features to select (default: 10% of features)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError    : If inputs are invalid or incompatible\n",
    "        \"\"\"\n",
    "        if (X.shape[0] != len(y)):\n",
    "            raise ValueError(\"Number of samples in X and y must match\")\n",
    "            \n",
    "        if (X.shape[1] != len(feature_names)):\n",
    "            raise ValueError(\"Number of features must match length of feature_names\")\n",
    "            \n",
    "        self.X             = X\n",
    "        self.y             = y\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features    = n_features or int(0.1 * X.shape[1])  # Default to 10% of features\n",
    "        \n",
    "        \n",
    "    def chi_square_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform chi-square feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Chi-square scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing chi-square feature selection...\")\n",
    "            \n",
    "            # Scale features to non-negative for chi-square\n",
    "            scaler            = MinMaxScaler()\n",
    "            X_scaled          = scaler.fit_transform(self.X.toarray())\n",
    "            \n",
    "            # Apply chi-square selection\n",
    "            selector          = SelectKBest(score_func = chi2, \n",
    "                                            k          = self.n_features)\n",
    "            \n",
    "            selector.fit(X_scaled, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using chi-square\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def information_gain_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform information gain feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Information gain scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing information gain selection...\")\n",
    "            \n",
    "            # Calculate mutual information scores\n",
    "            selector          = SelectKBest(score_func = mutual_info_classif, \n",
    "                                            k          = self.n_features)\n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using information gain\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def correlation_based_selection(self, threshold: float = 0.8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform correlation-based feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            threshold { float } : Correlation threshold for feature removal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "               { ndarray }      :  Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing correlation-based selection...\")\n",
    "            \n",
    "            # Convert sparse matrix to dense for correlation calculation\n",
    "            X_dense         = self.X.toarray()\n",
    "            \n",
    "            # Calculate correlation matrix\n",
    "            corr_matrix     = np.corrcoef(X_dense.T)\n",
    "            \n",
    "            # Find highly correlated feature pairs\n",
    "            high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "            \n",
    "            # Keep track of features to remove\n",
    "            to_remove       = set()\n",
    "            \n",
    "            # For each pair of highly correlated features\n",
    "            for i, j in zip(*high_corr_pairs):\n",
    "                if ((i != j) and (i not in to_remove) and (j not in to_remove)):\n",
    "                    # Calculate correlation with target for both features\n",
    "                    corr_i = mutual_info_score(X_dense[:, i], self.y)\n",
    "                    corr_j = mutual_info_score(X_dense[:, j], self.y)\n",
    "                    \n",
    "                    # Remove feature with lower correlation to target\n",
    "                    if (corr_i < corr_j):\n",
    "                        to_remove.add(i)\n",
    "                        \n",
    "                    else:\n",
    "                        to_remove.add(j)\n",
    "            \n",
    "            # Get selected features\n",
    "            all_features      = set(range(self.X.shape[1]))\n",
    "            selected_features = np.array(list(all_features - to_remove))\n",
    "            \n",
    "            # Select top k features if more than n_features remain\n",
    "            if (len(selected_features) > self.n_features):\n",
    "                # Calculate mutual information for remaining features\n",
    "                mi_scores         = mutual_info_classif(self.X[:, selected_features], self.y)\n",
    "                top_k_idx         = np.argsort(mi_scores)[::-1][:self.n_features]\n",
    "                selected_features = selected_features[top_k_idx]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using correlation-based selection\")\n",
    "            \n",
    "            return selected_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def recursive_feature_elimination(self, estimator = None, cv: int = 5) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform Recursive Feature Elimination with cross-validation\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator  : Classifier to use (default: LogisticRegression)\n",
    "            cv         : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple }  : Tuple containing: - Selected feature indices\n",
    "                                           - Feature importance rankings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing recursive feature elimination...\")\n",
    "            \n",
    "            # Use logistic regression if no estimator provided\n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            # Perform RFE with cross-validation\n",
    "            selector = RFECV(estimator              = estimator,\n",
    "                             min_features_to_select = self.n_features,\n",
    "                             cv                     = cv,\n",
    "                             n_jobs                 = -1)\n",
    "            \n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and rankings\n",
    "            selected_features = np.where(selector.support_)[0]\n",
    "            rankings          = selector.ranking_\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using RFE\")\n",
    "            \n",
    "            return selected_features, rankings\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "           \n",
    "        \n",
    "    def forward_selection(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing forward selection...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            selected_features  = list()\n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            for i in tqdm(range(self.n_features)):\n",
    "                best_score   = -np.inf\n",
    "                best_feature = None\n",
    "                \n",
    "                # Try adding each remaining feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = selected_features + [feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores = cross_val_score(estimator, \n",
    "                                             X_subset, \n",
    "                                             self.y,\n",
    "                                             cv      = cv, \n",
    "                                             scoring = 'accuracy')\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score   = avg_score\n",
    "                        best_feature = feature\n",
    "                \n",
    "                if (best_feature is not None):\n",
    "                    selected_features.append(best_feature)\n",
    "                    remaining_features.remove(best_feature)\n",
    "                \n",
    "            print(f\"Selected {len(selected_features)} features using forward selection\")\n",
    "            \n",
    "            return np.array(selected_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def backward_elimination(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform backward feature elimination\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing backward elimination...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            while len(remaining_features) > self.n_features:\n",
    "                best_score    = -np.inf\n",
    "                worst_feature = None\n",
    "                \n",
    "                # Try removing each feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = [f for f in remaining_features if f != feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores = cross_val_score(estimator, \n",
    "                                             X_subset, \n",
    "                                             self.y,\n",
    "                                             cv      = cv, \n",
    "                                             scoring = 'accuracy')\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score    = avg_score\n",
    "                        worst_feature = feature\n",
    "                \n",
    "                if worst_feature is not None:\n",
    "                    remaining_features.remove(worst_feature)\n",
    "            \n",
    "            print(f\"Selected {len(remaining_features)} features using backward elimination\")\n",
    "            return np.array(remaining_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ba8e8",
   "metadata": {},
   "source": [
    "### Initialize the feature selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d20f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = TextFeatureSelector(X             = combined_features,\n",
    "                               y             = imdb_ratings_df['sentiment'].values,\n",
    "                               feature_names = feature_names,\n",
    "                               n_features    = None,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa49e785",
   "metadata": {},
   "source": [
    "### Perform Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Square Selection\n",
    "chi_square_features, chi_square_scores = selector.chi_square_selection()\n",
    "\n",
    "# Information Gain Selection\n",
    "#ig_features, ig_scores                 = selector.information_gain_selection()\n",
    "\n",
    "# Correlation-Based Selection\n",
    "#corr_features                          = selector.correlation_based_selection()\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "#rfe_features, rfe_rankings             = selector.recursive_feature_elimination()\n",
    "\n",
    "# Forward Selection\n",
    "#forward_features                       = selector.forward_selection()\n",
    "\n",
    "# Backward Elimination\n",
    "#backward_features                      = selector.backward_elimination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba76f9",
   "metadata": {},
   "source": [
    "### Get selected features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ace710",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_combined_features  = combined_features[:, chi_square_features]\n",
    "\n",
    "#selected_combined_features = combined_features[:, ig_features]\n",
    "\n",
    "#selected_combined_features = combined_features[:, corr_features]\n",
    "\n",
    "#selected_combined_features = combined_features[:, rfe_features]\n",
    "\n",
    "#selected_combined_features = combined_features[:, forward_features]\n",
    "\n",
    "#selected_combined_features = combined_features[:, backward_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a84a7a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c11c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for training and evaluating sentiment analysis models, including testing on unseen data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, feature_eng, vectorizers, selected_feature_indices, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the SentimentAnalyzer by splitting the data.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            X                        : Feature matrix (sparse matrix or ndarray)\n",
    "            \n",
    "            y                        : Target labels (array-like)\n",
    "            \n",
    "            feature_eng              : Instance of TextFeatureEngineering\n",
    "            \n",
    "            vectorizers              : Tuple of vectorizers used for feature transformation\n",
    "            \n",
    "            selected_feature_indices : Indices of selected features after feature selection\n",
    "            \n",
    "            test_size                : Proportion of data to use for testing (default: 0.2)\n",
    "            \n",
    "            random_state             : Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size    = test_size, \n",
    "                                                                                random_state = random_state)\n",
    "        \n",
    "        self.feature_eng                                     = feature_eng\n",
    "        self.vectorizers                                     = vectorizers\n",
    "        self.selected_feature_indices                        = selected_feature_indices\n",
    "\n",
    "        \n",
    "    def train_model(self, model_type:str = \"logistic_regression\", kernel=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Train a sentiment analysis model.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model_type { str } : Type of model to train (e.g: \"logistic_regression\", \"svm\", \"random_forest\")\n",
    "            \n",
    "            kernel     { str } : Kernel type for SVM (e.g., \"linear\", \"poly\", \"rbf\", \"sigmoid\")\n",
    "            \n",
    "            kwargs             : Additional arguments for the model initialization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        if (model_type == \"logistic_regression\"):\n",
    "            model = LogisticRegression(max_iter = 1000, **kwargs)\n",
    "            \n",
    "        elif (model_type == \"svm\"):\n",
    "            \n",
    "            if (kernel is None):\n",
    "                # Default kernel\n",
    "                kernel = \"rbf\"  \n",
    "                \n",
    "            model = SVC(kernel = kernel, **kwargs)\n",
    "            \n",
    "        elif (model_type == \"random_forest\"):\n",
    "            model = RandomForestClassifier(**kwargs)\n",
    "            \n",
    "        elif model_type == \"naive_bayes\":\n",
    "            model = MultinomialNB(**kwargs)\n",
    "\n",
    "        elif model_type == \"lightgbm\":\n",
    "            model = LGBMClassifier(**kwargs)\n",
    "\n",
    "        elif model_type == \"logistic_model_tree\":\n",
    "            model = DecisionTreeClassifier(**kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model_type. Choose from: 'logistic_regression', 'svm', 'random_forest'\")\n",
    "\n",
    "        print(f\"Training {model_type}...\")\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        Evaluate a trained model on the test set\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model : Trained model\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        y_pred   = model.predict(self.X_test)\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        report   = classification_report(self.y_test, y_pred)\n",
    "        cm       = confusion_matrix(self.y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        return {\"accuracy\"              : accuracy,\n",
    "                \"classification_report\" : report,\n",
    "                \"confusion_matrix\"      : cm,\n",
    "               }\n",
    "\n",
    "    \n",
    "    def test_on_unseen_data(self, model, unseen_texts):\n",
    "        \"\"\"\n",
    "        Test the model on unseen data\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model         : Trained model\n",
    "            \n",
    "            unseen_texts  : List of unseen text data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Predictions for the unseen data\n",
    "        \"\"\"\n",
    "        print(\"Processing unseen data...\")\n",
    "\n",
    "        # Preprocess unseen data (implement preprocessing in the feature engineering class)\n",
    "        binary_features          = self.vectorizers[0].transform(unseen_texts)\n",
    "        tfidf_features           = self.vectorizers[1].transform(unseen_texts)\n",
    "        bm25_features            = self.vectorizers[2].transform(unseen_texts)\n",
    "\n",
    "        # Combine features\n",
    "        unseen_combined_features = hstack([binary_features, tfidf_features, bm25_features])\n",
    "\n",
    "        # Select features using the indices chosen during feature selection\n",
    "        unseen_selected_features = unseen_combined_features[:, self.selected_feature_indices]\n",
    "\n",
    "        # Predict sentiments\n",
    "        predictions              = model.predict(unseen_selected_features)\n",
    "\n",
    "        # Print predictions\n",
    "        print(\"Predictions on Unseen Data:\")\n",
    "        for text, pred in zip(unseen_texts, predictions):\n",
    "            print(f\"Text: {text}\\nPredicted Sentiment: {pred}\\n\")\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a95f9f",
   "metadata": {},
   "source": [
    "### Initialize the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentAnalyzer(X                        = selected_chi_squared_features, \n",
    "                             y                        = imdb_ratings_df[\"sentiment\"].values,\n",
    "                             feature_eng              = feature_eng,\n",
    "                             vectorizers              = (binary_vectorizer, tfidf_vectorizer, bm25_transformer),\n",
    "                             selected_feature_indices = chi_square_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0473d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\"This movie is speedy enough with plot twists, but hard to understand the connection between plots.\",\n",
    "             \"Seriously, this is the best movie I've ever watched! Everything was flawless!\",\n",
    "             \"The storyline was okay, but the acting was just not up to the mark.\",\n",
    "             \"A complete disaster of a movie. Don't waste your time.\",\n",
    "             \"I can't believe how amazing this was. Totally worth it!\",\n",
    "             \"The movie had its moments, but overall, it felt like something was missing.\",\n",
    "             \"I absolutely loved the cinematography, but the acting was subpar.\",\n",
    "             \"The film is an excellent example of how not to make a movie.\",\n",
    "             \"It's hard to imagine how anyone could dislike this masterpiece!\",\n",
    "             \"The trailer was better than the actual movie. Felt cheated.\",\n",
    "             \"A rollercoaster of emotions! Highly recommend watching this.\",\n",
    "             \"An average movie with nothing new to offer.\",\n",
    "             \"The pacing was terrible, and the climax was predictable.\",\n",
    "             \"Wow, just wow. This is how a movie should be made!\",\n",
    "             \"A decent watch for a lazy weekend. Not groundbreaking, but enjoyable.\",\n",
    "             \"The director has outdone themselves; what a phenomenal movie!\",\n",
    "             \"More hype than substance. A complete letdown.\",\n",
    "             \"Good visuals, decent music, but lacked a solid script.\",\n",
    "             \"A masterpiece in every sense. This will stay with me forever.\",\n",
    "             \"Mediocre at best. Not worth the ticket price.\",\n",
    "             \"A fresh take on a tired genre. Highly recommend it!\",\n",
    "             \"Overrated and boring. Nothing special about it.\",\n",
    "             \"This is one of those movies you'll regret missing. A must-watch!\",\n",
    "             \"Predictable plot, but the performances were top-notch.\",\n",
    "             \"It's a bad movie if you're looking for entertainment.\",\n",
    "             \"Can't believe I sat through the entire thing. A waste of time.\",\n",
    "             \"Finally, a movie that gets it right. Loved every minute of it!\",\n",
    "             \"A forgettable movie with no real impact.\",\n",
    "             \"An extraordinary journey that left me speechless. Bravo!\",\n",
    "             \"The humor was forced, and the dialogue was cringeworthy.\",\n",
    "             \"A solid movie with a gripping narrative. Well done!\",\n",
    "             \"The music was fantastic, but the rest of the movie was average.\",\n",
    "             \"Ironic how they managed to make something so beautiful look so bland.\",\n",
    "             \"An epic conclusion to a fantastic series. Couldnt have been better!\",\n",
    "             \"The movie tries too hard to be funny and fails miserably.\",\n",
    "             \"A fresh and engaging story with relatable characters.\",\n",
    "             \"All style, no substance. Disappointing.\",\n",
    "             \"A breath of fresh air! One of the best movies this year.\",\n",
    "             \"The plot was all over the place, but it was fun to watch.\",\n",
    "             \"Couldn't make it through the first half. Painful to sit through.\",\n",
    "             \"An unexpectedly beautiful film that touched my heart.\",\n",
    "             \"Trying to understand why this movie exists is more entertaining than the movie itself.\",\n",
    "             \"Every second of this movie was a blessing. Pure cinematic joy.\",\n",
    "             \"The lead actor was the only saving grace in an otherwise dull film.\",\n",
    "             \"A pretentious attempt at storytelling that falls flat.\",\n",
    "             \"I didnt expect much, but this movie surprised me in the best way.\",\n",
    "             \"A series of poorly executed clichs masquerading as a story.\",\n",
    "             \"This is not just a movie; its an experience. Brilliant!\",\n",
    "             \"A slog of a movie with a laughably bad ending.\",\n",
    "            ]\n",
    "\n",
    "\n",
    "sentiments = [\"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",  \n",
    "              \"positive\", \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",    \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",    \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",  \n",
    "              \"negative\", \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",  \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",  \n",
    "              \"positive\",    \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e124442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie is speedy enough with plot twists, but hard to understand the connection between plots.',\n",
       " \"Seriously, this is the best movie I've ever watched! Everything was flawless!\",\n",
       " 'The storyline was okay, but the acting was just not up to the mark.',\n",
       " \"A complete disaster of a movie. Don't waste your time.\",\n",
       " \"I can't believe how amazing this was. Totally worth it!\",\n",
       " 'The movie had its moments, but overall, it felt like something was missing.',\n",
       " 'I absolutely loved the cinematography, but the acting was subpar.',\n",
       " 'The film is an excellent example of how not to make a movie.',\n",
       " \"It's hard to imagine how anyone could dislike this masterpiece!\",\n",
       " 'The trailer was better than the actual movie. Felt cheated.',\n",
       " 'A rollercoaster of emotions! Highly recommend watching this.',\n",
       " 'An average movie with nothing new to offer.',\n",
       " 'The pacing was terrible, and the climax was predictable.',\n",
       " 'Wow, just wow. This is how a movie should be made!',\n",
       " 'A decent watch for a lazy weekend. Not groundbreaking, but enjoyable.',\n",
       " 'The director has outdone themselves; what a phenomenal movie!',\n",
       " 'More hype than substance. A complete letdown.',\n",
       " 'Good visuals, decent music, but lacked a solid script.',\n",
       " 'A masterpiece in every sense. This will stay with me forever.',\n",
       " 'Mediocre at best. Not worth the ticket price.',\n",
       " 'A fresh take on a tired genre. Highly recommend it!',\n",
       " 'Overrated and boring. Nothing special about it.',\n",
       " \"This is one of those movies you'll regret missing. A must-watch!\",\n",
       " 'Predictable plot, but the performances were top-notch.',\n",
       " \"It's a bad movie if you're looking for entertainment.\",\n",
       " \"Can't believe I sat through the entire thing. A waste of time.\",\n",
       " 'Finally, a movie that gets it right. Loved every minute of it!',\n",
       " 'A forgettable movie with no real impact.',\n",
       " 'An extraordinary journey that left me speechless. Bravo!',\n",
       " 'The humor was forced, and the dialogue was cringeworthy.',\n",
       " 'A solid movie with a gripping narrative. Well done!',\n",
       " 'The music was fantastic, but the rest of the movie was average.',\n",
       " 'Ironic how they managed to make something so beautiful look so bland.',\n",
       " 'An epic conclusion to a fantastic series. Couldnt have been better!',\n",
       " 'The movie tries too hard to be funny and fails miserably.',\n",
       " 'A fresh and engaging story with relatable characters.',\n",
       " 'All style, no substance. Disappointing.',\n",
       " 'A breath of fresh air! One of the best movies this year.',\n",
       " 'The plot was all over the place, but it was fun to watch.',\n",
       " \"Couldn't make it through the first half. Painful to sit through.\",\n",
       " 'An unexpectedly beautiful film that touched my heart.',\n",
       " 'Trying to understand why this movie exists is more entertaining than the movie itself.',\n",
       " 'Every second of this movie was a blessing. Pure cinematic joy.',\n",
       " 'The lead actor was the only saving grace in an otherwise dull film.',\n",
       " 'A pretentious attempt at storytelling that falls flat.',\n",
       " 'I didnt expect much, but this movie surprised me in the best way.',\n",
       " 'A series of poorly executed clichs masquerading as a story.',\n",
       " 'This is not just a movie; its an experience. Brilliant!',\n",
       " 'A slog of a movie with a laughably bad ending.',\n",
       " nan]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(pd.read_csv('../data/test_data.csv')['Text'])\n",
    "pd.to_csv(pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b588e2f",
   "metadata": {},
   "source": [
    "### Train a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb00328",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = analyzer.train_model(model_type = \"logistic_regression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93a503",
   "metadata": {},
   "source": [
    "### Evaluate the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = analyzer.evaluate_model(logistic_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20f8db",
   "metadata": {},
   "source": [
    "### Predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_predictions = analyzer.test_on_unseen_data(model        = logistic_model, \n",
    "                                                    unseen_texts = test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55382fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = {'texts' : test_data, 'true_labels' : sentiments, 'predicted_labels' : list(logistic_predictions)}\n",
    "\n",
    "total_test_df = pd.DataFrame.from_dict(data   = all_test_data, \n",
    "                                       orient = 'index').T\n",
    "\n",
    "total_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc401c30",
   "metadata": {},
   "source": [
    "### Train an SVM model with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_linear = analyzer.train_model(model_type = \"svm\", \n",
    "                                        kernel     = \"linear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b7d6c",
   "metadata": {},
   "source": [
    "### Evaluate the SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560693ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_evaluation_results = analyzer.evaluate_model(svm_model_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf67af9",
   "metadata": {},
   "source": [
    "### Predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffa80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear_predictions = analyzer.test_on_unseen_data(model        = svm_model_linear, \n",
    "                                                      unseen_texts = test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde62bcb",
   "metadata": {},
   "source": [
    "### Train an SVM model with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_rbf    = analyzer.train_model(model_type = \"svm\", \n",
    "                                        kernel     = \"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c401dc",
   "metadata": {},
   "source": [
    "### Evaluate the SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.evaluate_model(svm_model_rbf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ad89c",
   "metadata": {},
   "source": [
    "### Train an SVM model with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0483709",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf_predictions = analyzer.predict(svm_model_rbf, test_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98677910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b080b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42623b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
